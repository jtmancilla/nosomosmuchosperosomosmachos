---
title: "Preparación de los Datos Algas"
author: "nosomos muchos pero somos machos"
date: "17/11/2014"
output: html_document
---

```{r, warning=FALSE, message=FALSE, error=FALSE, echo=FALSE}

# Este bloque debería de tener la bandera de echo a FALSE

library(dplyr)
library(tidyr)
library(ggplot2)
library(lubridate)
library(stringr)

```



```{r, echo=FALSE}

# Este bloque debería de tener la bandera de echo a FALSE

# NOTA: Todos los bloques de este documento, tienen eval a FALSE, se debería de eliminar

# Cargamos el dataset
source("0-load.r")

ds.path <- "~/itam-dm/data/algas/" # Puede ser un URL o una dirección en el directorio

ds.name <- "algas.txt" # Nombre de nuestro conjunto de datos, e.g. algas, german

ds  <- load() # Leemos el dataset, usando readRDS (ya que ya lo tenemos en disco debido a nuestro EDA)
   # Hay que utilizar el data set al que no se le han removido las observaciones NAs
   # Lo guardamos en la variable ds para hacer más genérico los pasos.

ds <- tbl_df(ds) # Para obtener un print mejorado
```


# Introducción

Nuestro archivo a explorar procede del repositorio UCI Machine Learning Repository.
Datos que evaluan caraterísticas de cierto tipo de algas en alguna región de USA.

## Variables

Nuestro dataset está compuesto de 200 observarciones en 18 variables. 3 de las variables son categóricas y 15 son numércias. 8 de las variables numéricas presneta NA`s. 

Las categóricas son : Temporada, tamaño, velocidad.
Algunas numércias son: mxPH, Cl, NO3, NO4, oP4, PO4, etc.



# Estructura General

```{r, echo=TRUE}
ds
```


## Tamaño
```{r}
dim(ds)
```

## Columnas

```{r}
names(ds)
```

## Estructura

```{r}
str(ds)
```

_NOTA: Indicar si hay una discrepancia entre las clases de las variables en el data set y en su significado, i.e. fechas que no son fechas, si no factores, etc._

## Observaciones

```{r}
head(ds)
```

```{r}
tail(ds)
```

```{r, echo=FALSE}
#ds[sample(ds,6),]
```


# Sumario Estadístico

```{r}
summary(ds)
```


# Limpieza de metadatos
```{r,warning=FALSE, message=FALSE, error=FALSE,echo=FALSE}

ds$"PO4"  <- (ds$"PO4"-mean(ds$"PO4",na.rm=T))/sd(ds$"PO4",na.rm=T)
ds$"oPO4"  <-  (ds$oPO4 - mean(ds$oPO4,na.rm=T))/ sd(ds$oPO4,na.rm=T)
ds$"mxPH"  <- (ds$"mxPH"-mean(ds$"mxPH",na.rm=T))/sd(ds$"mxPH",na.rm=T)
ds$"mnO2"  <-  (ds$mnO2 - mean(ds$mnO2 ,na.rm=T))/ sd(ds$mnO2 ,na.rm=T)

ds$Cl  <- (ds$Cl-mean(ds$Cl,na.rm=T))/sd(ds$Cl,na.rm=T)
ds$NO3  <-  (ds$NO3 - mean(ds$NO3,na.rm=T))/ sd(ds$NO3,na.rm=T)
ds$NO4  <- (ds$NO4 -mean(ds$NO4,na.rm=T))/sd(ds$NO4,na.rm=T)
ds$Chla  <-  (ds$Chla - mean(ds$Chla ,na.rm=T))/ sd(ds$Chla ,na.rm=T)

ds$a1  <- (ds$a1-mean(ds$a1,na.rm=T))/sd(ds$a1,na.rm=T)
ds$a2  <-  (ds$a2 - mean(ds$a2,na.rm=T))/ sd(ds$a2,na.rm=T)
ds$a3  <- (ds$a3 -mean(ds$a3,na.rm=T))/sd(ds$a3,na.rm=T)
ds$a4  <-  (ds$a4 - mean(ds$a4,na.rm=T))/ sd(ds$a4 ,na.rm=T)
ds$a5  <-  (ds$a5 - mean(ds$a5,na.rm=T))/ sd(ds$a5,na.rm=T)
ds$a6  <- (ds$a6 -mean(ds$a6,na.rm=T))/sd(ds$a6,na.rm=T)
ds$a7  <-  (ds$a7 - mean(ds$a7,na.rm=T))/ sd(ds$a7,na.rm=T)

```


```{r}
source("~/itam-dm/alumnos/jtmancilla/german/utils.r")

# Usaremos la función que hiciste de ejercicio
names(ds) <- normalizarNombres(names(ds))
```


Además de normalizar los nombres de variables, este es el lugar para poner nombres que tengan significado como que la columna que tenga datos de fecha, se llame `fecha` o `date`.

```{r}
names(ds)
```

# Ajuste de formatos

Las clases de las variables son

```{r}
sapply(ds, class)
```


En esta sección arreglamos los formatos de los datos. Un ejemplo típico son las fechas.

Otros problemas con variables son: categóricas/numéricas que no lo son, booleanas que no lo son, ordenar variables nominales, reetiquetar las variables categóricas, etc.

Para arreglar las fechas, utiliza el paquete `lubridate`.

El formato de fechas debe de ser `YMD` y si es `timestamp` debe de serlo hasta la precisión que den los datos, no más, no menos.

```{r, eval=FALSE, echo=FALSE}
# Ejemplo hipotético

#ds$fecha <- ymd(as.character(ds$fecha))
```

*NOTA: Es recomendable hacer todas las transformaciones en un solo `mutate` y no una por una (a menos que haya problemas de memoria, y hay que usar otras técnicas).*

Así quedan las variables corregidas:

```{r}
sapply(ds, class) 
```

# Transformación de variables

En esta sección incluímos la transformación de las variables necesarias (normalización, estandarización, _binning_, `log`, etc.)


*NOTA: Es recomendable hacer todas las transformaciones en un solo `mutate` y no una por una (a menos que haya problemas de memoria, y hay que usar otras técnicas).*
# Identificación de variables

```{r}


# Aplicando Normalización:

normalizar  <- function(data){
    df  <- data.frame()
    for (n in 1:ncol(data)){
            if(class(data[,n])=="numeric"){
                data$n  <- (data[,n]-mean(data[,n],na.rm=T))/sd(data[,n],na.rm=T)
                
            }
    }
}


normalizar(ds)





vars <- names(ds) # Guardamos los nombres de variables



target <- "tamaño"  # Si el modelo es supervisado
#risk <- "" # Si se proveé, es la importancia de la observación respecto a la variable (es una variable de salida)
#costo <- "" # Costo de equivocarse en la predicción (Si se proveé) (es una variable de salida)
id <- vars # Armar una id con columnas, o seleccionar el id del dataset
```

# Recodificación

No aplica para este caso..

Antes de pasar a la etapa de ignorar variables, es importante **recodificar**. 

- Hay métodos como el  `randomForest` que no soporta variables categóricas con más de 32 niveles, habría que agruparlos (e.g. si son países se pueden reagrupar por región, similitud -esto requiere otra base de datos, etc.)

- Si las fechas son `timestamp` hay que extraer variables categóricas como `mes`, `día.de.la.semana`, `fin.de.semana`, `temporada`, etc. claro que depende del tipo del problema al que nos estemos enfrentando.


# Variables a ignorar

No aplicaca en este caso..

Identificamos en una variable, las columnas a ignorar en el entrenamiento del modelo.

### IDs y variables de salida

```{r}
vars.a.ignorar <- id #union(id,if (exists("risk")) risk, if (exists("costo")) costo)
```

### Constantes y valores únicos por observación

```{r, eval=FALSE}
# Ignoramos las que tengan un único valor por cada observación, pueden ser IDs
# IMPORTANTE: Esto puede eliminar fechas, ver sección anterior

ids <- names(which(sapply(ds, function(x) length(unique(x)) == nrow(ds))))

# Ignoramos los factores que tengan muchos niveles
# IMPORTANTE: ver sección anterior

factors <- which(sapply(ds[vars], is.factor))
niveles <- sapply(factors, function(x) length(levels(ds[[x]])))
(muchos.niveles <- names(which(niveles > 20)))

vars.a.ignorar <- union(vars.a.ignorar, muchos.niveles)

# Constantes
constantes <- names(which(sapply(ds[vars], function(x) all(x == x[1L]))))

var.a.ignorar <- union(vars.a.ignorar, ids, constantes)
```


### Faltantes

```{r}
# Las que sean puros NAs
ids.nas.count <- sapply(ds[vars], function(x) sum(is.na(x)))
ids.nas <- names(which(ids.nas.count == nrow(ds)))

vars.a.ignorar <- ids.nas #union(ids.nas, vars.a.ignorar)

# Las que tengan muchos NAs (un 70% o más)
ids.many.nas <- names(which(ids.nas.count >= 0.7*nrow(ds)))

vars.a.ignorar <- union(ids.many.nas, vars.a.ignorar)
```

### Variable de salida (`target`) 

Si el problema de minado, es supervisado, removemos las observaciones que tengan `NA` en la variable `target`

```{r}
dim(ds)
ds <- ds[!is.na(ds[target]),]
dim(ds)
```

Si el problema es de clasificación, hay que convertir la variable `target` a categórica.

```{r}
ds[target] <- as.factor(ds[[target]])
table(ds[target])
```

Mostramos la distribución (esto nos indicará si el problema no está balanceado)

```{r}
ggplot(data=ds, aes_string(x=target)) + geom_bar(width=0.3)
```


# Variables correlacionadas

```{r}
vars.cor <- cor(ds[which(sapply(ds, is.numeric))], use="complete.obs")
vars.cor[upper.tri(vars.cor, diag=TRUE)] <- NA

vars.cor <- vars.cor                                  %>%
            abs()                                     %>%   
            data.frame()                              %>%
            mutate(var1=row.names(vars.cor))          %>%
            gather(var2, cor, -var1)                  %>%
            na.omit()
            

vars.cor <- vars.cor[order(-abs(vars.cor$cor)), ]

(muy.cor <- filter(vars.cor, cor > 0.95)) # Mostramos las que tengan más del 95% de correlación

# Habría que decidir si se remueven y cuales se remueven (var1 o var2)
vars.a.ignorar <- union(vars.a.ignorar, muy.cor$var2)
```

En esta base de observación no tenemos ninguna variable con una correlación igual o mayro
al 95%.

_NOTA: ¿Qué pasa con las categóricas? ¿Usamos asociación o independencia?_


# Valores faltantes

Se elimiarán las observaciones: 62 y 199 por tener un alto % de NA en el registro.

```{r}
observaciones.omitidas  <- ds[apply(ds, 1, function(x) sum(is.na(x))) > 2,]

ds <- ds[-c(199,62),]
```



Para los valores faltantes de la columna PO4 utilizaremos una regresión lineal, utilizando
a oPO4 como covariable.
```{r}

lm  <- predict(lm(po4 ~ o.po4, data=ds))
ds$po4 <-  ifelse(!is.na(ds[,"po4"]),ds[,"po4"],lm)

```

Para el resto de las columnas con NA`s se imputara con el promedio.
```{r,echo=FALSE}
# cambio de NA por mean
ds$"mx.ph"   <-  ifelse(!is.na(ds$"mx.ph"),ds$"mx.ph",mean(ds$"mx.ph",na.rm=T))
ds$"mn.o2"   <-  ifelse(!is.na(ds$"mn.o2"),ds$"mn.o2",mean(ds$"mn.o2",na.rm=T))

ds$"cl"   <-  ifelse(!is.na(ds$"cl"),ds$"cl",mean(ds$"cl",na.rm=T))
ds$"chla"   <-  ifelse(!is.na(ds$"chla"),ds$"chla",mean(ds$"chla",na.rm=T))  
```


```{r}
#Creando función

imputarValorCentral <- function(data, colnames){
    for(n in colnames){
        if(class(data[,n])=="factor"){
            
            vec  <- ifelse(!is.na(data[,n]),data[,n],
                             names(table(data[,n]))[max(table(data[,n])) == table(data[,n])])
            
            data[,n]   <-  mapvalues(vec,from=1:nlevels(data[,n]), to=levels(data[,n]))  
        }
        else{
            data[,n]   <-  ifelse(!is.na(data[,n]),data[,n],mean(data[,n],na.rm=T))
            
        }
    }
}

imputarValorCentral(ds,c("mx.ph","mn.o2","cl","chla"))                    



```


# Normalizar niveles

Removemos espacios, puntuaciones, camelCase, etc. en los niveles de los factores supervivientes.

```{r}

factors <- which(sapply(ds[vars], is.factor))
for (f in factors) levels(ds[[f]]) <- normalizarNombres(levels(ds[[f]]))
```

```{r, eval=TRUE, echo=FALSE}

# Este paso debería de tener la bandera de echo a FALSE

# Removemos las variables
vars <- setdiff(vars, vars.a.ignorar)

```

# Identificación de Variables

```{r, eval=TRUE}
(vars.input <- setdiff(vars, target))
idxs.input <- sapply(vars.input, function(x) which(x == names(ds)), USE.NAMES=FALSE)

idxs.numericas <- intersect(idxs.input, which(sapply(ds, is.numeric)))
(vars.numericas <- names(ds)[idxs.numericas])

idxs.categoricas <- intersect(idxs.input, which(sapply(ds, is.factor)))
(vars.categoricas <- names(ds)[idxs.categoricas])

# Por conveniencia guardamos el número de observaciones supervivientes
num.observaciones <- nrow(ds)

```

```{r, eval=TRUE, echo=FALSE}

# Este paso debería de tener la bandera de echo a FALSE

# Guardamos todo en la carpeta 
ds.date <- paste0("_", format(Sys.Date(), "%y%m%d"))
ds.rdata <- paste0(ds.name, ds.date, ".RData") # Guardamos todo en un RData para poder automatizar el modelado

if (!file.exists("clean")) dir.create("clean") # Creamos la carpeta clean, si no existe

# save(ds, ds.name, ds.path, ds.date, target,# risk, costo, 
#      id, vars.a.ignorar, vars, num.observaciones, 
#      vars.input, idxs.input,
#      observaciones.omitidas,
#      vars.numericas, idxs.numericas,
#      vars.categoricas, idxs.categoricas,
#      file=paste0("/", "clean", ds.rdata)
#      )
```


### Apéndice: Ambiente

```{r, echo=FALSE}
sessionInfo()
```

